# OmniParser: Comprehensive Workflow & Architecture
**Universal Content Ingestion Platform**

**Version:** 2.0 (Expanded Vision)
**Date:** 2025-10-17
**Status:** Architecture Design

---

## Table of Contents
1. [System Overview](#system-overview)
2. [Input Source Taxonomy](#input-source-taxonomy)
3. [Parser Routing & Selection](#parser-routing--selection)
4. [Processing Pipeline](#processing-pipeline)
5. [Integration Patterns](#integration-patterns)
6. [Future Roadmap](#future-roadmap)

---

## System Overview

### Complete End-to-End Flow

```mermaid
graph TB
    subgraph InputSources["INPUT SOURCES (20+ Types)"]
        direction LR

        subgraph Documents["ğŸ“„ Documents"]
            EPUB[EPUB Books]
            PDF[PDFs]
            DOCX[Word Docs]
            ODT[LibreOffice]
            RTF[Rich Text]
        end

        subgraph Web["ğŸŒ Web Content"]
            URL[Website URLs]
            News[News Articles]
            Blogs[Blog Posts]
            Recipes[Recipe Sites]
            Docs[Documentation]
            Wiki[Wikis]
        end

        subgraph Social["ğŸ“± Social Media"]
            Twitter[Twitter/X Posts]
            Reddit[Reddit Threads]
            LinkedIn[LinkedIn Articles]
            Medium[Medium Posts]
            Substack[Substack Newsletters]
        end

        subgraph Feeds["ğŸ“¡ Feeds & APIs"]
            RSS[RSS/Atom Feeds]
            Email[Email Newsletters]
            JSON[JSON APIs]
            XML[XML Data]
        end

        subgraph Archives["ğŸ“¦ Archives"]
            ZIP[ZIP Files]
            TAR[TAR Archives]
            Multi[Multi-file Collections]
        end

        subgraph Cloud["â˜ï¸ Cloud Storage"]
            GDocs[Google Docs]
            Notion[Notion Pages]
            Confluence[Confluence]
            Dropbox[Dropbox Paper]
        end

        subgraph Code["ğŸ’» Code & Tech"]
            Markdown[Markdown Files]
            README[README Files]
            Jupyter[Jupyter Notebooks]
            TechDocs[Technical Docs]
        end

        subgraph Other["ğŸ“‹ Other"]
            TXT[Plain Text]
            CSV[Structured Data]
            Custom[Custom Formats]
        end
    end

    subgraph OmniParser["ğŸ¯ OMNIPARSER CORE"]
        direction TB

        Detect[Format Detection<br/>â€¢ MIME types<br/>â€¢ Content sniffing<br/>â€¢ API identification]

        Route[Parser Selection<br/>â€¢ 15+ parsers<br/>â€¢ Format-specific<br/>â€¢ Fallback chain]

        Extract[Content Extraction<br/>â€¢ Text parsing<br/>â€¢ Structure detection<br/>â€¢ Metadata extraction]

        Process[Post-Processing<br/>â€¢ Chapter detection<br/>â€¢ Image extraction<br/>â€¢ Text cleaning<br/>â€¢ Markdown conversion]

        Build[Document Assembly<br/>â€¢ Unified structure<br/>â€¢ Standardized output<br/>â€¢ Quality validation]

        Detect --> Route
        Route --> Extract
        Extract --> Process
        Process --> Build
    end

    subgraph Consumers["ğŸ“¤ CONSUMERS & INTEGRATIONS"]
        direction LR

        subgraph Audio["ğŸ§ Audio Generation"]
            TTS[epub2tts<br/>Audiobook Creation]
            Podcast[Podcast Generation]
        end

        subgraph AI["ğŸ¤– AI & Knowledge"]
            RAG[RAG Systems<br/>Knowledge Bases]
            LLM[LLM Fine-tuning<br/>Dataset Creation]
            Embeddings[Vector Embeddings<br/>Semantic Search]
        end

        subgraph Content["ğŸ“ Content Management"]
            CMS[Content Systems<br/>Publishing]
            Archive[Digital Archives<br/>Preservation]
            Translation[Translation Services<br/>i18n]
        end

        subgraph Analysis["ğŸ“Š Analysis & Tools"]
            Search[Search Indexing<br/>Elasticsearch]
            Summary[Summarization<br/>AI Tools]
            Analytics[Content Analytics<br/>Insights]
        end
    end

    Documents --> Detect
    Web --> Detect
    Social --> Detect
    Feeds --> Detect
    Archives --> Detect
    Cloud --> Detect
    Code --> Detect
    Other --> Detect

    Build --> TTS
    Build --> Podcast
    Build --> RAG
    Build --> LLM
    Build --> Embeddings
    Build --> CMS
    Build --> Archive
    Build --> Translation
    Build --> Search
    Build --> Summary
    Build --> Analytics

    style InputSources fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    style OmniParser fill:#fff3e0,stroke:#f57c00,stroke-width:4px
    style Consumers fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px

    style Detect fill:#a5d6a7
    style Route fill:#90caf9
    style Extract fill:#ffcc80
    style Process fill:#ce93d8
    style Build fill:#81c784
```

---

## Input Source Taxonomy

### Detailed Source Classification

```mermaid
mindmap
  root((OmniParser<br/>Input Sources))
    Documents
      Books & Publications
        EPUB E-books
        PDF Documents
        Text-based PDFs
        Scanned PDFs OCR
        PDF Forms
      Office Documents
        Microsoft Word DOCX
        LibreOffice ODT
        Rich Text Format RTF
        WordPerfect WPD
      Academic
        Research Papers
        Thesis Documents
        Citations & References
        LaTeX outputs
    Web Content
      Websites
        HTML Pages
        Single Page Apps SPA
        Server Side Rendered SSR
        JavaScript Heavy Sites
      News & Media
        News Articles
        Press Releases
        Online Magazines
        Investigative Reports
      Personal Publishing
        Blog Posts
        Personal Websites
        Portfolio Sites
        Recipe Sites
        How-to Guides
      Technical
        Documentation Sites
        API References
        GitHub Pages
        ReadTheDocs
        GitBook
        Confluence
      Community
        Forums
        Discussion Boards
        Q&A Sites Stack Overflow
        Wiki Pages Wikipedia
    Social Media
      Microblogging
        Twitter X Posts
        Twitter Threads
        Mastodon Toots
        Bluesky Posts
      Professional
        LinkedIn Articles
        LinkedIn Posts
        GitHub Discussions
      Blogging Platforms
        Medium Articles
        Substack Newsletters
        Dev.to Posts
        Hashnode Articles
      Community Platforms
        Reddit Posts
        Reddit Threads
        Hacker News Stories
        Product Hunt Posts
    Feeds & Subscriptions
      Syndication
        RSS Feeds
        Atom Feeds
        JSON Feed
      Email
        Email Newsletters
        Mailing List Archives
        Email Digests
        Automated Reports
      Notifications
        Webhook Payloads
        Event Streams
        Real-time Updates
    Archives & Collections
      Compressed Files
        ZIP Archives
        TAR Balls
        RAR Files
        7-Zip
      Multi-file
        Batch Uploads
        Directory Structures
        Project Folders
        Document Collections
    Cloud & Collaboration
      Google Workspace
        Google Docs
        Google Slides
        Google Sheets
      Microsoft 365
        OneDrive Files
        SharePoint Docs
        Teams Messages
      Note-taking
        Notion Pages
        Evernote Notes
        OneNote Notebooks
        Obsidian Vaults
      Project Management
        Confluence Pages
        Jira Descriptions
        Asana Tasks
        Trello Cards
    Code & Technical
      Markdown
        README files
        Documentation MD
        GitHub Markdown
        GitLab Flavored
      Notebooks
        Jupyter Notebooks
        Google Colab
        Observable Notebooks
      Code Documentation
        Docstrings
        JSDoc Comments
        Swagger OpenAPI
        GraphQL Schemas
    Structured Data
      Data Formats
        JSON Documents
        XML Files
        YAML Config
        TOML Files
        CSV Data
      API Responses
        REST API JSON
        GraphQL Responses
        SOAP XML
        gRPC Protobuf
    Specialized
      Scientific
        arXiv Papers
        PubMed Articles
        Academic Databases
      Legal
        Legal Documents
        Court Filings
        Contracts
      Financial
        Financial Reports
        SEC Filings
        Annual Reports
      Medical
        Medical Records
        Research Studies
        Patient Information
```

---

## Parser Routing & Selection

### Intelligent Parser Selection Flow

```mermaid
flowchart TD
    Start([Input Received<br/>file_path | URL | API data])

    CheckType{Input Type?}

    Start --> CheckType

    %% File Path Branch
    CheckType -->|File Path| ValidateFile{File<br/>Exists &<br/>Readable?}
    ValidateFile -->|No| ErrFile([FileReadError])
    ValidateFile -->|Yes| DetectFormat[Detect Format<br/>â€¢ Magic bytes MIME<br/>â€¢ Extension fallback<br/>â€¢ Content sampling]

    %% URL Branch
    CheckType -->|URL| ValidateURL{Valid<br/>URL?}
    ValidateURL -->|No| ErrURL([ValidationError])
    ValidateURL -->|Yes| FetchContent[Fetch Content<br/>â€¢ HTTP/HTTPS<br/>â€¢ User-agent<br/>â€¢ Timeout handling]
    FetchContent --> DetectWeb{Content<br/>Type?}

    DetectWeb -->|HTML| WebParser
    DetectWeb -->|JSON| JSONParser
    DetectWeb -->|XML| XMLParser
    DetectWeb -->|RSS/Atom| FeedParser
    DetectWeb -->|Other| ContentSniff[Content<br/>Sniffing]
    ContentSniff --> DetectFormat

    %% API Data Branch
    CheckType -->|API Data| DetectAPI{Data<br/>Format?}
    DetectAPI -->|JSON| JSONParser
    DetectAPI -->|XML| XMLParser
    DetectAPI -->|Text| TextParser

    %% Format Detection to Parser Selection
    DetectFormat --> SelectParser{Format<br/>Identified?}

    SelectParser -->|epub| EPUBParser[ğŸ“• EPUB Parser<br/>ebooklib-based<br/>TOC detection<br/>Chapter extraction]
    SelectParser -->|pdf| PDFParser[ğŸ“„ PDF Parser<br/>PyMuPDF + OCR<br/>Text/scanned<br/>Table extraction]
    SelectParser -->|docx| DOCXParser[ğŸ“ DOCX Parser<br/>python-docx<br/>Style-based chapters<br/>Image extraction]
    SelectParser -->|odt| ODTParser[ğŸ“„ ODT Parser<br/>LibreOffice<br/>ODF structure<br/>Metadata]
    SelectParser -->|rtf| RTFParser[ğŸ“„ RTF Parser<br/>striprtf<br/>Basic formatting<br/>Text extraction]
    SelectParser -->|html| WebParser[ğŸŒ Web Parser<br/>Trafilatura<br/>Readability<br/>Main content]
    SelectParser -->|markdown| MarkdownParser[ğŸ“ Markdown Parser<br/>Frontmatter<br/>Heading detection<br/>Code blocks]
    SelectParser -->|txt| TextParser[ğŸ“„ Text Parser<br/>chardet encoding<br/>Minimal processing<br/>Single chapter]
    SelectParser -->|json| JSONParser[ğŸ”§ JSON Parser<br/>Schema detection<br/>Structured extraction<br/>Field mapping]
    SelectParser -->|xml| XMLParser[ğŸ”§ XML Parser<br/>lxml<br/>XPath queries<br/>Structure parsing]
    SelectParser -->|csv| CSVParser[ğŸ“Š CSV Parser<br/>pandas<br/>Column detection<br/>Tabular data]
    SelectParser -->|zip/tar| ArchiveParser[ğŸ“¦ Archive Parser<br/>Extract & iterate<br/>Nested parsing<br/>Batch processing]
    SelectParser -->|jupyter| NotebookParser[ğŸ’» Notebook Parser<br/>nbformat<br/>Cell extraction<br/>Code + markdown]
    SelectParser -->|unknown| FallbackChain[Fallback Chain<br/>1. Try text parser<br/>2. Try encoding detection<br/>3. Sample-based guess]

    FallbackChain -->|Success| TextParser
    FallbackChain -->|Fail| ErrUnsupported([UnsupportedFormatError])

    %% Social Media Special Branch
    CheckType -->|Social Media| DetectPlatform{Platform?}
    DetectPlatform -->|Twitter/X| TwitterParser[ğŸ¦ Twitter Parser<br/>API v2<br/>Thread detection<br/>Media extraction]
    DetectPlatform -->|Reddit| RedditParser[ğŸ”´ Reddit Parser<br/>PRAW<br/>Comment threads<br/>Submission content]
    DetectPlatform -->|LinkedIn| LinkedInParser[ğŸ’¼ LinkedIn Parser<br/>Web scraping<br/>Article extraction<br/>Post content]
    DetectPlatform -->|Medium| MediumParser[ğŸ“° Medium Parser<br/>Article scraping<br/>Paywall handling<br/>Rich content]

    %% Feed Branch
    CheckType -->|Feed URL| FeedParser[ğŸ“¡ Feed Parser<br/>feedparser<br/>RSS/Atom<br/>Entry extraction]

    %% All parsers converge to processing
    EPUBParser --> PostProcess
    PDFParser --> PostProcess
    DOCXParser --> PostProcess
    ODTParser --> PostProcess
    RTFParser --> PostProcess
    WebParser --> PostProcess
    MarkdownParser --> PostProcess
    TextParser --> PostProcess
    JSONParser --> PostProcess
    XMLParser --> PostProcess
    CSVParser --> PostProcess
    ArchiveParser --> PostProcess
    NotebookParser --> PostProcess
    TwitterParser --> PostProcess
    RedditParser --> PostProcess
    LinkedInParser --> PostProcess
    MediumParser --> PostProcess
    FeedParser --> PostProcess

    PostProcess[ğŸ“‹ Post-Processing<br/>â€¢ Chapter detection<br/>â€¢ Metadata extraction<br/>â€¢ Image processing<br/>â€¢ Text cleaning]

    PostProcess --> BuildDoc[ğŸ—ï¸ Build Document<br/>â€¢ Universal structure<br/>â€¢ Validation<br/>â€¢ Quality checks]

    BuildDoc --> Output([ğŸ“¦ Document Object<br/>â€¢ content markdown<br/>â€¢ chapters List<br/>â€¢ images List<br/>â€¢ metadata<br/>â€¢ processing_info])

    %% Styling
    style Start fill:#e8f5e9
    style Output fill:#e8f5e9
    style ErrFile fill:#ffebee
    style ErrURL fill:#ffebee
    style ErrUnsupported fill:#ffebee
    style PostProcess fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style BuildDoc fill:#e1f5fe,stroke:#0277bd,stroke-width:2px

    %% Parser styling
    style EPUBParser fill:#c8e6c9,stroke:#388e3c
    style PDFParser fill:#ffccbc,stroke:#d84315
    style DOCXParser fill:#b3e5fc,stroke:#0277bd
    style WebParser fill:#ffe0b2,stroke:#ef6c00
    style MarkdownParser fill:#f8bbd0,stroke:#c2185b
    style TextParser fill:#d1c4e9,stroke:#512da8
    style JSONParser fill:#ffecb3,stroke:#f57f17
    style XMLParser fill:#c5cae9,stroke:#283593
    style TwitterParser fill:#b2ebf2,stroke:#00838f
    style RedditParser fill:#ffcdd2,stroke:#c62828
    style MediumParser fill:#c5e1a5,stroke:#558b2f
    style FeedParser fill:#dcedc8,stroke:#689f38
```

---

## Processing Pipeline

### Detailed Processing Stages

```mermaid
flowchart TB
    subgraph Stage1["STAGE 1: INPUT VALIDATION"]
        Input[Raw Input<br/>File | URL | Data]

        Check1{File<br/>Exists?}
        Check2{Size<br/>Valid?}
        Check3{Format<br/>Supported?}
        Check4{Encoding<br/>Valid?}

        Input --> Check1
        Check1 -->|Yes| Check2
        Check1 -->|No| Err1([FileReadError])
        Check2 -->|Yes| Check3
        Check2 -->|No| Err2([ValidationError:<br/>File too large])
        Check3 -->|Yes| Check4
        Check3 -->|No| Err3([UnsupportedFormatError])
        Check4 -->|Yes| Valid1[âœ“ Validated Input]
        Check4 -->|No| Err4([EncodingError])
    end

    subgraph Stage2["STAGE 2: FORMAT DETECTION & PARSER SELECTION"]
        Valid1 --> Detect[Format Detection]

        Detect --> MagicBytes[Magic Bytes<br/>MIME Type]
        Detect --> Extension[File Extension<br/>Fallback]
        Detect --> ContentSample[Content Sampling<br/>First 1KB]

        MagicBytes --> Confidence{Confidence<br/>>80%?}
        Extension --> Confidence
        ContentSample --> Confidence

        Confidence -->|Yes| SelectParser[Select Parser]
        Confidence -->|No| TryFallback[Try Fallback Chain]

        TryFallback --> SelectParser

        SelectParser --> Valid2[âœ“ Parser Selected]
    end

    subgraph Stage3["STAGE 3: CONTENT EXTRACTION"]
        Valid2 --> Extract[Parser-Specific<br/>Extraction]

        Extract --> RawContent[Raw Content<br/>Bytes/Text/HTML]
        Extract --> RawMetadata[Raw Metadata<br/>Headers/Properties]
        Extract --> RawStructure[Raw Structure<br/>TOC/Outline/DOM]
        Extract --> RawMedia[Raw Media<br/>Images/Embeds]

        RawContent --> Valid3[âœ“ Content Extracted]
        RawMetadata --> Valid3
        RawStructure --> Valid3
        RawMedia --> Valid3
    end

    subgraph Stage4["STAGE 4: CONTENT TRANSFORMATION"]
        Valid3 --> Transform[Content<br/>Transformation]

        Transform --> HTMLtoMD{HTML to<br/>Markdown?}
        Transform --> EncodingFix[Encoding Fix<br/>ftfy]
        Transform --> WhitespaceFix[Whitespace<br/>Normalization]
        Transform --> CharFix[Character<br/>Normalization]

        HTMLtoMD -->|Yes| MDConvert[Markdown<br/>Conversion]
        HTMLtoMD -->|No| KeepFormat[Keep Format]

        MDConvert --> Valid4[âœ“ Content Transformed]
        KeepFormat --> Valid4
        EncodingFix --> Valid4
        WhitespaceFix --> Valid4
        CharFix --> Valid4
    end

    subgraph Stage5["STAGE 5: STRUCTURE DETECTION"]
        Valid4 --> DetectStruct[Structure<br/>Detection]

        DetectStruct --> ChapterDetect[Chapter Detection<br/>â€¢ Heading-based<br/>â€¢ TOC-based<br/>â€¢ Pattern-based<br/>â€¢ Heuristic]

        DetectStruct --> HierarchyBuild[Hierarchy Building<br/>â€¢ Level detection<br/>â€¢ Parent-child<br/>â€¢ Nesting]

        DetectStruct --> BoundaryCalc[Boundary Calculation<br/>â€¢ Start positions<br/>â€¢ End positions<br/>â€¢ Word counts]

        ChapterDetect --> Valid5[âœ“ Structure Detected]
        HierarchyBuild --> Valid5
        BoundaryCalc --> Valid5
    end

    subgraph Stage6["STAGE 6: METADATA EXTRACTION"]
        Valid5 --> ExtractMeta[Metadata<br/>Extraction]

        ExtractMeta --> CoreMeta[Core Metadata<br/>â€¢ Title<br/>â€¢ Author<br/>â€¢ Date<br/>â€¢ Language]

        ExtractMeta --> ExtendedMeta[Extended Metadata<br/>â€¢ Publisher<br/>â€¢ ISBN/DOI<br/>â€¢ Tags<br/>â€¢ Description]

        ExtractMeta --> TechMeta[Technical Metadata<br/>â€¢ File size<br/>â€¢ Format<br/>â€¢ Version<br/>â€¢ Encoding]

        ExtractMeta --> CustomMeta[Custom Metadata<br/>â€¢ Source URL<br/>â€¢ Platform<br/>â€¢ API data<br/>â€¢ User fields]

        CoreMeta --> Valid6[âœ“ Metadata Extracted]
        ExtendedMeta --> Valid6
        TechMeta --> Valid6
        CustomMeta --> Valid6
    end

    subgraph Stage7["STAGE 7: MEDIA PROCESSING"]
        Valid6 --> ProcessMedia[Media<br/>Processing]

        ProcessMedia --> ImageExtract[Image Extraction<br/>â€¢ Inline images<br/>â€¢ Embedded<br/>â€¢ Linked]

        ProcessMedia --> ImageMeta[Image Metadata<br/>â€¢ Alt text<br/>â€¢ Captions<br/>â€¢ Dimensions<br/>â€¢ Format]

        ProcessMedia --> ImagePosition[Position Tracking<br/>â€¢ Character offset<br/>â€¢ Chapter relation<br/>â€¢ Context]

        ProcessMedia --> ImageStorage[Storage Handling<br/>â€¢ Save to disk<br/>â€¢ Base64 encode<br/>â€¢ URL reference]

        ImageExtract --> Valid7[âœ“ Media Processed]
        ImageMeta --> Valid7
        ImagePosition --> Valid7
        ImageStorage --> Valid7
    end

    subgraph Stage8["STAGE 8: TEXT CLEANING"]
        Valid7 --> CleanText[Text Cleaning]

        CleanText --> RemovePatterns[Remove Patterns<br/>â€¢ Footnotes [1]<br/>â€¢ URLs<br/>â€¢ Artifacts<br/>â€¢ Junk]

        CleanText --> TransformPatterns[Transform Patterns<br/>â€¢ Em dashes<br/>â€¢ Quotes<br/>â€¢ Special chars<br/>â€¢ Ligatures]

        CleanText --> NormalizeWS[Normalize Whitespace<br/>â€¢ Extra spaces<br/>â€¢ Line breaks<br/>â€¢ Indentation<br/>â€¢ Tabs]

        CleanText --> FixTypography[Fix Typography<br/>â€¢ Smart quotes<br/>â€¢ Apostrophes<br/>â€¢ Ellipsis<br/>â€¢ Dashes]

        RemovePatterns --> Valid8[âœ“ Text Cleaned]
        TransformPatterns --> Valid8
        NormalizeWS --> Valid8
        FixTypography --> Valid8
    end

    subgraph Stage9["STAGE 9: QUALITY VALIDATION"]
        Valid8 --> Validate[Quality<br/>Validation]

        Validate --> CheckContent{Content<br/>Not Empty?}
        Validate --> CheckChapters{Chapters<br/>Detected?}
        Validate --> CheckMeta{Metadata<br/>Present?}
        Validate --> CheckEncoding{Encoding<br/>Valid UTF-8?}

        CheckContent -->|No| Warn1[âš  Warning:<br/>Empty content]
        CheckChapters -->|No| Warn2[âš  Warning:<br/>No chapters]
        CheckMeta -->|No| Warn3[âš  Warning:<br/>Missing metadata]
        CheckEncoding -->|No| Warn4[âš  Warning:<br/>Encoding issues]

        CheckContent -->|Yes| Valid9[âœ“ Quality Validated]
        CheckChapters -->|Yes| Valid9
        CheckMeta -->|Yes| Valid9
        CheckEncoding -->|Yes| Valid9

        Warn1 --> Valid9
        Warn2 --> Valid9
        Warn3 --> Valid9
        Warn4 --> Valid9
    end

    subgraph Stage10["STAGE 10: DOCUMENT ASSEMBLY"]
        Valid9 --> Assemble[Document<br/>Assembly]

        Assemble --> CreateDoc[Create Document<br/>Object]

        CreateDoc --> SetContent[Set Content<br/>â€¢ Full markdown<br/>â€¢ Word count<br/>â€¢ Reading time]

        CreateDoc --> SetChapters[Set Chapters<br/>â€¢ Chapter list<br/>â€¢ Hierarchy<br/>â€¢ Boundaries]

        CreateDoc --> SetImages[Set Images<br/>â€¢ Image list<br/>â€¢ References<br/>â€¢ Metadata]

        CreateDoc --> SetMetadata[Set Metadata<br/>â€¢ Core fields<br/>â€¢ Extended fields<br/>â€¢ Custom fields]

        CreateDoc --> SetProcessing[Set Processing Info<br/>â€¢ Parser used<br/>â€¢ Timestamp<br/>â€¢ Options<br/>â€¢ Warnings]

        SetContent --> FinalDoc[ğŸ“¦ Document Object]
        SetChapters --> FinalDoc
        SetImages --> FinalDoc
        SetMetadata --> FinalDoc
        SetProcessing --> FinalDoc
    end

    FinalDoc --> Return([Return to Caller])

    %% Styling
    style Stage1 fill:#ffebee,stroke:#c62828
    style Stage2 fill:#e3f2fd,stroke:#1565c0
    style Stage3 fill:#f3e5f5,stroke:#6a1b9a
    style Stage4 fill:#fff3e0,stroke:#e65100
    style Stage5 fill:#e8f5e9,stroke:#2e7d32
    style Stage6 fill:#fce4ec,stroke:#c2185b
    style Stage7 fill:#e0f2f1,stroke:#00695c
    style Stage8 fill:#fff9c4,stroke:#f57f17
    style Stage9 fill:#ede7f6,stroke:#4527a0
    style Stage10 fill:#e1f5fe,stroke:#0277bd

    style FinalDoc fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px
    style Return fill:#a5d6a7,stroke:#1b5e20,stroke-width:2px
```

---

## Integration Patterns

### How Consumers Use OmniParser

```mermaid
graph TB
    subgraph Input["INPUT STAGE"]
        User[User/Application]
        Files[Local Files]
        URLs[Web URLs]
        APIs[API Data]
        Streams[Data Streams]
    end

    subgraph OmniParser["OMNIPARSER"]
        Parse[parse_document]
        Doc[Document Object]
    end

    User --> Parse
    Files --> Parse
    URLs --> Parse
    APIs --> Parse
    Streams --> Parse

    Parse --> Doc

    subgraph Consumers["CONSUMER INTEGRATIONS"]
        direction TB

        subgraph Pattern1["Pattern 1: Direct Integration"]
            App1[Application Code]

            App1 -->|1. Import| Import1[from omniparser<br/>import parse_document]
            Import1 -->|2. Call| Call1[doc = parse_document]
            Call1 -->|3. Use| Use1[Access doc.content<br/>doc.chapters<br/>doc.metadata]

            Use1 --> Process1[Application-specific<br/>Processing]
        end

        subgraph Pattern2["Pattern 2: Batch Processing"]
            Batch[Batch Processor]

            Batch -->|1. Iterate| Loop[for file in directory]
            Loop -->|2. Parse| Parse2[doc = parse_document file]
            Parse2 -->|3. Store| Store[Save to database/<br/>file system]
            Store -->|4. Log| Log[Track progress/<br/>errors]
        end

        subgraph Pattern3["Pattern 3: Pipeline Integration"]
            Pipeline[Data Pipeline]

            Pipeline -->|1. Fetch| Fetch[Fetch content<br/>from source]
            Fetch -->|2. Parse| Parse3[doc = parse_document]
            Parse3 -->|3. Transform| Transform[Custom<br/>transformations]
            Transform -->|4. Load| Load[Load to<br/>destination]
        end

        subgraph Pattern4["Pattern 4: Microservice"]
            Service[OmniParser<br/>Microservice]

            Service -->|1. Receive| API[HTTP API<br/>Request]
            API -->|2. Parse| Parse4[parse_document]
            Parse4 -->|3. Serialize| JSON[Convert to JSON]
            JSON -->|4. Return| Response[HTTP Response]
        end

        subgraph Pattern5["Pattern 5: Event-Driven"]
            Events[Event System]

            Events -->|1. Trigger| Event[Document uploaded<br/>event]
            Event -->|2. Process| Parse5[parse_document]
            Parse5 -->|3. Publish| Publish[Publish parsed<br/>event]
            Publish -->|4. Consume| Subscribers[Downstream<br/>consumers]
        end
    end

    Doc --> Pattern1
    Doc --> Pattern2
    Doc --> Pattern3
    Doc --> Pattern4
    Doc --> Pattern5

    subgraph Examples["REAL-WORLD EXAMPLES"]
        direction LR

        Example1[epub2tts<br/>Audiobook Generation]
        Example2[RAG System<br/>Knowledge Base]
        Example3[Content CMS<br/>Publishing Platform]
        Example4[Search Engine<br/>Document Indexing]
        Example5[Translation Service<br/>i18n Pipeline]
        Example6[Summarization<br/>AI Tool]
        Example7[Archive System<br/>Digital Preservation]
        Example8[Learning Platform<br/>Course Materials]
    end

    Process1 --> Example1
    Store --> Example2
    Load --> Example3
    Response --> Example4
    Subscribers --> Example5
    Transform --> Example6
    Store --> Example7
    Process1 --> Example8

    style Input fill:#e8f5e9
    style OmniParser fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    style Consumers fill:#e3f2fd
    style Examples fill:#f3e5f5

    style Doc fill:#ffeb3b,stroke:#f57c00,stroke-width:2px
```

### Specific Use Case: epub2tts Integration

```mermaid
sequenceDiagram
    autonumber

    participant User as User
    participant Web as epub2tts Web UI
    participant Backend as epub2tts Backend
    participant OP as OmniParser
    participant TTS as TTS Engine
    participant Audio as Audio Output

    User->>Web: Upload document<br/>(EPUB, PDF, DOCX, URL, etc.)
    Web->>Backend: POST /process<br/>file + options

    rect rgb(230, 245, 255)
        Note over Backend,OP: Document Parsing Phase
        Backend->>OP: parse_document(file_path,<br/>extract_images=True)

        OP->>OP: Detect format<br/>(auto-detect type)
        OP->>OP: Select parser<br/>(EPUB/PDF/DOCX/Web/etc.)
        OP->>OP: Extract content<br/>chapters + metadata + images
        OP->>OP: Post-process<br/>clean + structure

        OP-->>Backend: Document object<br/>(content, chapters, metadata, images)
    end

    rect rgb(255, 243, 224)
        Note over Backend,TTS: TTS Processing Phase
        Backend->>Backend: Apply TTS-specific cleaning<br/>(pause markers, dialogue detection)
        Backend->>Backend: Split into audio chunks<br/>(respect chapter boundaries)

        loop For each chapter
            Backend->>TTS: Generate audio<br/>chapter.content
            TTS-->>Backend: Audio chunk<br/>(MP3/M4A)
        end

        Backend->>Backend: Stitch audio chunks<br/>(create audiobook)
        Backend->>Backend: Add chapter markers<br/>(from doc.chapters)
        Backend->>Backend: Embed metadata<br/>(from doc.metadata)
    end

    rect rgb(232, 245, 233)
        Note over Backend,Audio: Output Generation
        Backend->>Audio: Save audiobook<br/>(M4B with chapters)
        Audio-->>Backend: File path

        Backend-->>Web: Processing complete<br/>download link
        Web-->>User: Audiobook ready<br/>(original format â†’ audio)
    end

    Note over User,Audio: âœ¨ Universal Input Format Support<br/>One pipeline handles EPUB, PDF, DOCX,<br/>web articles, blog posts, and more!
```

---

## Future Roadmap

### Expansion Phases (Beyond v1.0)

```mermaid
gantt
    title OmniParser Development Roadmap
    dateFormat YYYY-MM-DD
    section Foundation
    Phase 1: Core Architecture           :done, p1, 2025-10-16, 1w
    Phase 2: EPUB Parser (epub2tts port) :active, p2, 2025-10-17, 3w
    Phase 3: Testing & Documentation     :p3, after p2, 2w

    section v1.0 Release
    v1.0: Initial Release                :milestone, m1, after p3, 1d
    EPUB Parser Only                     :crit, after p3, 1d

    section v1.1 - Core Formats
    PDF Parser (PyMuPDF + OCR)           :p4, after m1, 2w
    DOCX Parser (python-docx)            :p5, after p4, 1w
    HTML/Web Parser (Trafilatura)        :p6, after p5, 1w
    v1.1: Core Formats Complete          :milestone, m2, after p6, 1d

    section v1.2 - Extended Formats
    Markdown Parser                      :p7, after m2, 3d
    Text Parser (encoding detection)     :p8, after p7, 2d
    RTF Parser                          :p9, after p8, 4d
    ODT Parser (LibreOffice)            :p10, after p9, 5d
    v1.2: Extended Formats              :milestone, m3, after p10, 1d

    section v1.3 - Structured Data
    JSON Parser (structured data)        :p11, after m3, 1w
    XML Parser (lxml-based)             :p12, after p11, 1w
    CSV Parser (pandas)                 :p13, after p12, 3d
    YAML Parser                         :p14, after p13, 2d
    v1.3: Structured Data Support       :milestone, m4, after p14, 1d

    section v1.4 - Web & Social
    Feed Parser (RSS/Atom)              :p15, after m4, 1w
    Twitter/X Parser (API v2)           :p16, after p15, 1w
    Reddit Parser (PRAW)                :p17, after p16, 1w
    Medium Parser (web scraping)        :p18, after p17, 5d
    LinkedIn Parser                     :p19, after p18, 5d
    v1.4: Social Media Support          :milestone, m5, after p19, 1d

    section v1.5 - Cloud & Collaboration
    Google Docs Parser                  :p20, after m5, 1w
    Notion Parser                       :p21, after p20, 1w
    Confluence Parser                   :p22, after p21, 1w
    Archive Parser (ZIP/TAR)            :p23, after p22, 1w
    v1.5: Cloud Platforms               :milestone, m6, after p23, 1d

    section v1.6 - Advanced Features
    Jupyter Notebook Parser             :p24, after m6, 1w
    Table Extraction (all formats)      :p25, after p24, 2w
    Advanced Image Processing           :p26, after p25, 1w
    Code Documentation Parser           :p27, after p26, 1w
    v1.6: Advanced Features             :milestone, m7, after p27, 1d

    section v2.0 - Intelligence
    AI-Powered Chapter Detection        :p28, after m7, 2w
    Semantic Content Analysis           :p29, after p28, 2w
    Auto-Tagging & Categorization       :p30, after p29, 1w
    Content Summarization               :p31, after p30, 1w
    Multi-Language Support (NLP)        :p32, after p31, 2w
    v2.0: AI-Enhanced Processing        :milestone, m8, after p32, 1d

    section v2.1 - Enterprise
    Streaming API (large files)         :p33, after m8, 2w
    Batch Processing API                :p34, after p33, 1w
    Webhook Integration                 :p35, after p34, 1w
    Progress Callbacks                  :p36, after p35, 1w
    Microservice Deployment             :p37, after p36, 1w
    v2.1: Enterprise Features           :milestone, m9, after p37, 1d

    section v2.2 - Ecosystem
    Plugin System                       :p38, after m9, 2w
    Custom Parser SDK                   :p39, after p38, 2w
    Parser Marketplace                  :p40, after p39, 3w
    Community Contributions             :p41, after p40, 4w
    v2.2: Ecosystem Platform            :milestone, m10, after p41, 1d
```

### Parser Priority Matrix

```mermaid
graph TD
    subgraph HighPriority["ğŸ”´ HIGH PRIORITY - v1.0-1.2"]
        HP1[EPUB Parser<br/>âœ… Phase 2.2 Active]
        HP2[PDF Parser<br/>ğŸ“„ Universal format]
        HP3[DOCX Parser<br/>ğŸ“ Office docs]
        HP4[HTML/Web Parser<br/>ğŸŒ Web content]
        HP5[Markdown Parser<br/>ğŸ“ Developer docs]
        HP6[Text Parser<br/>ğŸ“„ Fallback option]
    end

    subgraph MediumPriority["ğŸŸ¡ MEDIUM PRIORITY - v1.3-1.5"]
        MP1[JSON Parser<br/>ğŸ”§ API data]
        MP2[XML Parser<br/>ğŸ”§ Structured data]
        MP3[RSS/Feed Parser<br/>ğŸ“¡ Syndication]
        MP4[Twitter/X Parser<br/>ğŸ¦ Social media]
        MP5[Reddit Parser<br/>ğŸ”´ Community content]
        MP6[Archive Parser<br/>ğŸ“¦ Batch processing]
        MP7[RTF Parser<br/>ğŸ“„ Legacy docs]
        MP8[ODT Parser<br/>ğŸ“„ Open formats]
    end

    subgraph LowPriority["ğŸŸ¢ LOW PRIORITY - v1.6+"]
        LP1[LinkedIn Parser<br/>ğŸ’¼ Professional content]
        LP2[Medium Parser<br/>ğŸ“° Blog platform]
        LP3[Google Docs Parser<br/>â˜ï¸ Cloud docs]
        LP4[Notion Parser<br/>ğŸ“ Knowledge base]
        LP5[Confluence Parser<br/>ğŸ“š Team wiki]
        LP6[Jupyter Parser<br/>ğŸ’» Notebooks]
        LP7[CSV Parser<br/>ğŸ“Š Tabular data]
        LP8[YAML Parser<br/>âš™ï¸ Config files]
    end

    subgraph FuturePriority["ğŸ”µ FUTURE - v2.0+"]
        FP1[Substack Parser<br/>ğŸ“§ Newsletters]
        FP2[GitHub Parser<br/>ğŸ’» Code repos]
        FP3[Telegram Parser<br/>ğŸ’¬ Messaging]
        FP4[Discord Parser<br/>ğŸ® Community]
        FP5[Slack Parser<br/>ğŸ’¼ Workplace]
        FP6[Email Parser<br/>ğŸ“§ Messages]
        FP7[Academic DB Parser<br/>ğŸ“ Research]
        FP8[Legal Doc Parser<br/>âš–ï¸ Specialized]
    end

    HighPriority -.->|After v1.2| MediumPriority
    MediumPriority -.->|After v1.5| LowPriority
    LowPriority -.->|After v1.6| FuturePriority

    style HighPriority fill:#ffebee,stroke:#c62828,stroke-width:3px
    style MediumPriority fill:#fff9c4,stroke:#f57f17,stroke-width:2px
    style LowPriority fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    style FuturePriority fill:#e3f2fd,stroke:#1565c0,stroke-width:2px
```

### Feature Evolution Map

```mermaid
mindmap
  root((OmniParser<br/>Evolution))
    v1.0 Foundation
      Core Architecture
        BaseParser interface
        Document model
        Exception handling
      Single Parser
        EPUB only
        TOC detection
        Basic metadata
      Testing
        Unit tests >80%
        Integration tests
        Fixtures
    v1.1-1.2 Format Expansion
      Document Parsers
        PDF PyMuPDF + OCR
        DOCX python-docx
        RTF striprtf
        ODT odfpy
      Web Parsers
        HTML Trafilatura
        Markdown frontmatter
        Text chardet
      Quality
        >85% coverage
        Performance tuning
        Error handling
    v1.3-1.5 Integration
      Structured Data
        JSON schema detection
        XML lxml
        CSV pandas
        YAML PyYAML
      Social Media
        Twitter API v2
        Reddit PRAW
        Medium scraping
        LinkedIn parsing
      Cloud Platforms
        Google Docs API
        Notion API
        Confluence API
        Dropbox Paper
      Archives
        ZIP support
        TAR support
        Nested parsing
        Batch processing
    v1.6 Advanced Features
      Enhanced Extraction
        Table extraction
        Form parsing
        Code block detection
        Equation extraction
      Media Processing
        Image enhancement
        OCR improvements
        Video metadata
        Audio transcripts
      Technical Parsers
        Jupyter notebooks
        LaTeX documents
        Swagger/OpenAPI
        GraphQL schemas
    v2.0 AI-Powered
      Intelligence
        AI chapter detection
        Semantic analysis
        Auto-summarization
        Entity extraction
      NLP Features
        Multi-language
        Translation hints
        Sentiment analysis
        Topic modeling
      Quality Enhancements
        Content scoring
        Completeness checks
        Accuracy validation
        Auto-correction
    v2.1+ Enterprise
      Performance
        Streaming API
        Chunked processing
        Parallel parsing
        Caching layer
      Integration
        Webhooks
        Event system
        Progress callbacks
        Real-time updates
      Deployment
        Microservice mode
        Docker containers
        Kubernetes support
        Cloud-native
      Monitoring
        Metrics
        Logging
        Tracing
        Alerts
    v2.2+ Ecosystem
      Extensibility
        Plugin system
        Custom parsers
        Parser SDK
        Community marketplace
      Developer Tools
        CLI tools
        VS Code extension
        Web playground
        API explorer
      Community
        Open source parsers
        Shared configs
        Best practices
        Use case library
```

---

## Technical Specifications

### Parser Capability Matrix

| Parser | Status | Formats | Chapter Detection | Metadata | Images | Tables | Links | Code Blocks | Priority |
|--------|--------|---------|-------------------|----------|--------|--------|-------|-------------|----------|
| **EPUB** | ğŸ—ï¸ In Progress | .epub | âœ… TOC-based | âœ… OPF | âœ… Yes | âš ï¸ Basic | âœ… Yes | âš ï¸ Basic | ğŸ”´ Critical |
| **PDF** | ğŸ“‹ Planned v1.1 | .pdf | âœ… Font-based | âœ… Properties | âœ… Yes | âœ… Advanced | âœ… Yes | âš ï¸ Basic | ğŸ”´ High |
| **DOCX** | ğŸ“‹ Planned v1.1 | .docx, .doc | âœ… Style-based | âœ… Core Props | âœ… Yes | âœ… Advanced | âœ… Yes | âœ… Yes | ğŸ”´ High |
| **HTML/Web** | ğŸ“‹ Planned v1.1 | .html, URLs | âœ… Heading-based | âš ï¸ Meta tags | âœ… Yes | âœ… Yes | âœ… Yes | âœ… Yes | ğŸ”´ High |
| **Markdown** | ğŸ“‹ Planned v1.2 | .md, .markdown | âœ… Heading-based | âœ… Frontmatter | âœ… Yes | âœ… Yes | âœ… Yes | âœ… Yes | ğŸ”´ High |
| **Text** | ğŸ“‹ Planned v1.2 | .txt | âš ï¸ Heuristic | âŒ None | âŒ No | âŒ No | âš ï¸ URL detection | âŒ No | ğŸŸ¡ Medium |
| **RTF** | ğŸ“‹ Planned v1.2 | .rtf | âš ï¸ Basic | âš ï¸ Limited | âš ï¸ Limited | âš ï¸ Limited | âš ï¸ Limited | âŒ No | ğŸŸ¢ Low |
| **ODT** | ğŸ“‹ Planned v1.2 | .odt | âœ… Style-based | âœ… Yes | âœ… Yes | âœ… Yes | âœ… Yes | âœ… Yes | ğŸŸ¢ Low |
| **JSON** | ğŸ“‹ Planned v1.3 | .json | âš ï¸ Schema-based | âœ… Structured | âš ï¸ Embedded | âš ï¸ Embedded | âœ… Yes | âœ… Yes | ğŸŸ¡ Medium |
| **XML** | ğŸ“‹ Planned v1.3 | .xml | âš ï¸ XPath-based | âœ… Attributes | âš ï¸ Embedded | âš ï¸ Embedded | âœ… Yes | âš ï¸ CDATA | ğŸŸ¡ Medium |
| **RSS/Atom** | ğŸ“‹ Planned v1.3 | RSS, Atom feeds | âœ… Per-item | âœ… Feed metadata | âœ… Yes | âŒ No | âœ… Yes | âš ï¸ Limited | ğŸŸ¡ Medium |
| **CSV** | ğŸ“‹ Planned v1.3 | .csv | âŒ Row-based | âš ï¸ Headers | âŒ No | âœ… Native | âŒ No | âŒ No | ğŸŸ¢ Low |
| **Twitter/X** | ğŸ“‹ Planned v1.4 | API | âš ï¸ Thread-based | âœ… User/tweet | âœ… Yes | âŒ No | âœ… Yes | âš ï¸ Limited | ğŸŸ¡ Medium |
| **Reddit** | ğŸ“‹ Planned v1.4 | API | âœ… Comment threads | âœ… Submission | âœ… Yes | âŒ No | âœ… Yes | âœ… Yes | ğŸŸ¡ Medium |
| **LinkedIn** | ğŸ“‹ Planned v1.4 | Web/API | âš ï¸ Heuristic | âœ… Profile | âœ… Yes | âŒ No | âœ… Yes | âš ï¸ Limited | ğŸŸ¢ Low |
| **Medium** | ğŸ“‹ Planned v1.4 | Web | âœ… Heading-based | âœ… Author/article | âœ… Yes | âš ï¸ Limited | âœ… Yes | âœ… Yes | ğŸŸ¢ Low |
| **Google Docs** | ğŸ“‹ Planned v1.5 | API | âœ… Style-based | âœ… Doc properties | âœ… Yes | âœ… Yes | âœ… Yes | âœ… Yes | ğŸŸ¢ Low |
| **Notion** | ğŸ“‹ Planned v1.5 | API | âœ… Block-based | âœ… Page properties | âœ… Yes | âœ… Yes | âœ… Yes | âœ… Yes | ğŸŸ¢ Low |
| **Confluence** | ğŸ“‹ Planned v1.5 | API/Web | âœ… Heading-based | âœ… Page metadata | âœ… Yes | âœ… Yes | âœ… Yes | âœ… Yes | ğŸŸ¢ Low |
| **ZIP/Archive** | ğŸ“‹ Planned v1.5 | .zip, .tar | âœ… Per-file | âš ï¸ Aggregate | âœ… Yes | âœ… Yes | âœ… Yes | âœ… Yes | ğŸŸ¡ Medium |
| **Jupyter** | ğŸ“‹ Planned v1.6 | .ipynb | âœ… Cell-based | âœ… Notebook meta | âœ… Yes | âœ… Yes | âœ… Yes | âœ… Yes | ğŸŸ¢ Low |

**Legend:**
- âœ… Fully Supported
- âš ï¸ Partially Supported / Basic Implementation
- âŒ Not Supported
- ğŸ—ï¸ In Progress
- ğŸ“‹ Planned
- ğŸ”´ Critical Priority
- ğŸŸ¡ Medium Priority
- ğŸŸ¢ Low Priority

---

## Configuration & Customization

### Parser Options Reference

```yaml
# Example: Universal parser options structure
parse_document:
  # Input options
  input:
    file_path: "path/to/file" or "https://url" or data
    encoding: "auto" # or specific encoding
    fallback_encoding: "utf-8"

  # Format detection
  format_detection:
    use_magic_bytes: true
    use_extension: true
    use_content_sniffing: true
    confidence_threshold: 0.8

  # Processing options
  processing:
    extract_images: true
    image_format: "reference" # or "base64" or "save"
    image_directory: "./images"

    detect_chapters: true
    chapter_detection_method: "auto" # toc, heading, pattern, heuristic
    min_chapter_length: 100 # words

    clean_text: true
    cleaning_patterns: "config/cleaning_patterns.yaml"
    remove_footnotes: true
    normalize_whitespace: true
    fix_encoding: true

    extract_metadata: true
    metadata_sources: ["native", "heuristic", "custom"]

    extract_tables: false # advanced feature
    preserve_code_blocks: true
    preserve_links: true

  # Output options
  output:
    format: "Document" # standard object
    markdown_flavor: "gfm" # GitHub Flavored Markdown
    include_toc: false
    include_reading_time: true

  # Performance options
  performance:
    max_file_size: "500MB"
    timeout: 300 # seconds
    parallel_parsing: false # for archives
    cache_enabled: false

  # Error handling
  error_handling:
    on_error: "raise" # or "warn" or "skip"
    collect_warnings: true
    verbose_errors: true

  # Parser-specific options
  parser_specific:
    epub:
      use_toc: true
      use_spine_fallback: true
      extract_cover: true

    pdf:
      ocr_enabled: true
      ocr_language: "eng"
      extract_tables: false
      detect_scanned: true

    web:
      use_trafilatura: true
      use_readability: true # fallback
      fetch_timeout: 30
      user_agent: "OmniParser/2.0"
      javascript_enabled: false # future: playwright

    social:
      twitter:
        include_replies: false
        include_retweets: false
        thread_detection: true
      reddit:
        include_comments: true
        max_comment_depth: 5
        sort_by: "best"
```

---

## Performance Benchmarks

### Target Performance Goals

```mermaid
graph LR
    subgraph InputSize["Input Size Categories"]
        Small[Small<br/>< 1 MB<br/>~50 pages]
        Medium[Medium<br/>1-10 MB<br/>50-500 pages]
        Large[Large<br/>10-100 MB<br/>500-5000 pages]
        XLarge[Extra Large<br/>100-500 MB<br/>5000+ pages]
    end

    subgraph Performance["Performance Targets"]
        T1[< 1 second<br/>âœ… Real-time]
        T2[< 5 seconds<br/>âœ… Acceptable]
        T3[< 30 seconds<br/>âš ï¸ Slow]
        T4[< 5 minutes<br/>âš ï¸ Very Slow]
    end

    subgraph Memory["Memory Usage"]
        M1[< 100 MB<br/>âœ… Minimal]
        M2[< 500 MB<br/>âœ… Acceptable]
        M3[< 2 GB<br/>âš ï¸ High]
        M4[< 5 GB<br/>âŒ Too High]
    end

    Small --> T1
    Small --> M1

    Medium --> T2
    Medium --> M2

    Large --> T3
    Large --> M3

    XLarge --> T4
    XLarge --> M3

    style T1 fill:#c8e6c9
    style T2 fill:#c8e6c9
    style T3 fill:#fff9c4
    style T4 fill:#fff9c4

    style M1 fill:#c8e6c9
    style M2 fill:#c8e6c9
    style M3 fill:#fff9c4
    style M4 fill:#ffcdd2
```

---

## Conclusion

This comprehensive workflow demonstrates **OmniParser's vision as a universal content ingestion platform**:

### Key Capabilities

1. **Universal Input Support**: 20+ input sources (documents, web, social, feeds, cloud, code, data)
2. **Intelligent Routing**: Automatic format detection and parser selection
3. **Standardized Output**: Unified `Document` model across all formats
4. **Extensible Architecture**: Easy to add new parsers and features
5. **Production Ready**: Built on proven code from epub2tts

### Strategic Value

- **For epub2tts**: Enables universal input (not just EPUB)
- **For RAG Systems**: Consistent document structure for knowledge bases
- **For Content Platforms**: Unified ingestion pipeline
- **For Developers**: Clean API, extensive documentation, active development

### Next Steps

1. **Phase 2.2**: Complete EPUB parser extraction (in progress)
2. **v1.1**: Add PDF, DOCX, HTML/Web parsers
3. **v1.2-1.3**: Expand to structured data and social media
4. **v1.4-1.5**: Cloud platforms and archives
5. **v2.0+**: AI-powered features and enterprise capabilities

**OmniParser: Parse Anything. Output Consistency. Build Everywhere.**

---

**Document Version:** 2.0
**Last Updated:** 2025-10-17
**Status:** Comprehensive Vision
**Next Action:** Complete EPUB parser (Phase 2.2)
